
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>AI SLAM Summary · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-multipart/multipart.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-image-captions/image-captions.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-disqus/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-donate/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-advanced-emoji/emoji-website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-terminal/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-codeblock-filename/block.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    


    

        
    
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="slam-with-objects-using-a-nonparametric-pose-graph.html" />
    
    
    <link rel="prev" href="Recursive_Bayes_Filter.html" />
    

    
    <link rel="stylesheet" href="gitbook/gitbook-plugin-chart/c3/c3.min.css">
    <script src="gitbook/gitbook-plugin-chart/c3/d3.min.js"></script>
    <script src="gitbook/gitbook-plugin-chart/c3/c3.min.js"></script>
    

    <script src="gitbook/gitbook-plugin-graph/d3.min.js"></script>
    <script src="gitbook/gitbook-plugin-graph/function-plot.js"></script>    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">Overview</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    About
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="SLAM-Introduction.html">
            
                <a href="SLAM-Introduction.html">
            
                    
                    SLAM Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="SLAM-Resources.html">
            
                <a href="SLAM-Resources.html">
            
                    
                    SLAM Resources
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="Software.html">
            
                <a href="Software.html">
            
                    
                    SLAM Software
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Math Foundamentals</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="Coordinate_System.html">
            
                <a href="Coordinate_System.html">
            
                    
                    Coordinate_System
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="Recursive_Bayes_Filter.html">
            
                <a href="Recursive_Bayes_Filter.html">
            
                    
                    Recursive Bayes Filter
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">AI SLAM</li>
        
        
    
        <li class="chapter active" data-level="3.1" data-path="AI-SLAM-Summary.html">
            
                <a href="AI-SLAM-Summary.html">
            
                    
                    AI SLAM Summary
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="slam-with-objects-using-a-nonparametric-pose-graph.html">
            
                <a href="slam-with-objects-using-a-nonparametric-pose-graph.html">
            
                    
                    Slam with objects using a nonparametric pose graph
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="Fusion++.html">
            
                <a href="Fusion++.html">
            
                    
                    Fusion++
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.4" data-path="SLAM++.html">
            
                <a href="SLAM++.html">
            
                    
                    SLAM++
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.5" data-path="Monocular-Object-and-Plane-SLAM-in-Structured-Environments.html">
            
                <a href="Monocular-Object-and-Plane-SLAM-in-Structured-Environments.html">
            
                    
                    Monocular Object and Plane SLAM in Structured Environments
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.6" data-path="CubeSLAM.html">
            
                <a href="CubeSLAM.html">
            
                    
                    CubeSLAM
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.7" data-path="SSFM.html">
            
                <a href="SSFM.html">
            
                    
                    SSFM
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Traditional SLAM</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="ORB-SLAM.html">
            
                <a href="ORB-SLAM.html">
            
                    
                    ORB-SLAM
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">lidar</li>
        
        
    
        <li class="chapter " data-level="5.1" data-path="lidar.html">
            
                <a href="lidar.html">
            
                    
                    lidar
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.2" data-path="graph-slam.html">
            
                <a href="graph-slam.html">
            
                    
                    graph slam
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">ML,DL</li>
        
        
    
        <li class="chapter " data-level="6.1" data-path="Mask-RCNN.html">
            
                <a href="Mask-RCNN.html">
            
                    
                    Mask-RCNN
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Tools</li>
        
        
    
        <li class="chapter " data-level="7.1" data-path="ros.html">
            
                <a href="ros.html">
            
                    
                    ros
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.2" data-path="kinect2.html">
            
                <a href="kinect2.html">
            
                    
                    kinect2
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Video Courses - Robot & SLAM & AI</li>
        
        
    
        <li class="chapter " data-level="8.1" data-path="小象学院slam无人驾驶学习笔记.html">
            
                <a href="小象学院slam无人驾驶学习笔记.html">
            
                    
                    小象学院slam无人驾驶学习笔记
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.2" data-path="机器人视觉与控制学习笔记.html">
            
                <a href="机器人视觉与控制学习笔记.html">
            
                    
                    机器人视觉与控制学习笔记
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="8.3" data-path="slam-course-robot-mapping.html">
            
                <a href="slam-course-robot-mapping.html">
            
                    
                    SLAM-Course-Robot Mapping
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >AI SLAM Summary</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="2018">2018</h1>
<h2 id="mid-fusion-octree-based-object-level-multi-instance-dynamic-slam">MID-Fusion: Octree-based Object-Level Multi-Instance Dynamic SLAM</h2>
<ul>
<li><a href="https://youtu.be/Be_Ndceb2VI" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="640" height="555" src="https://www.youtube.com/embed/Be_Ndceb2VI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<pre><code class="lang-bibtex">@article{xu2018mid,
  title={MID-Fusion: Octree-based Object-Level Multi-Instance Dynamic SLAM},
  author={Xu, Binbin and Li, Wenbin and Tzoumanikas, Dimos and Bloesch, Michael and Davison, Andrew and Leutenegger, Stefan},
  journal={arXiv preprint arXiv:1812.07976},
  year={2018}
}
</code></pre>
<h2 id="fusion-volumetric-object-level-slam">Fusion++: Volumetric Object-Level SLAM</h2>
<p>&#x4F53;&#x79EF;&#x5BF9;&#x8C61;&#x7EA7;SLAM</p>
<ul>
<li><p><a href="https://www.doc.ic.ac.uk/~sleutene/publications/fusion_plusplus_3dv_camera_ready.pdf" target="_blank">Paper-PDF</a></p>
</li>
<li><p><a href="https://github.com/matterport/Mask_RCNN" target="_blank">matterport/Mask_RCNN</a>
This is an implementation of Mask R-CNN on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It&apos;s based on Feature Pyramid Network (FPN) and a ResNet101 backbone.</p>
</li>
<li><a href="https://github.com/andyzeng/tsdf-fusion" target="_blank">andyzeng/tsdf-fusion</a>
CUDA/C++ code to fuse multiple registered depth maps into a projective truncated signed distance function (TSDF) voxel volume, which can then be used to create high quality 3D surface meshes and point clouds. Tested on Ubuntu 14.04 and 16.04.</li>
</ul>
<ul>
<li><p><a href="https://github.com/tensorpack/tensorpack" target="_blank">tensorpack/tensorpack</a></p>
</li>
<li><p><a href="https://www.youtube.com/embed/2luKNC03x4k" target="_blank">Demo-Youtube</a></p>
</li>
</ul>
<iframe width="640" height="480" src="https://www.youtube.com/embed/2luKNC03x4k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<ul>
<li>cite:</li>
</ul>
<p>McCormac, John, et al. &quot;Fusion++: Volumetric object-level slam.&quot; 2018 International Conference on 3D Vision (3DV). IEEE, 2018.</p>
<ul>
<li>Authors</li>
</ul>
<p>John McCormac&#x2217; Ronald Clark&#x2217; Michael Bloesch Andrew J. Davison Stefan Leutenegger
Dyson Robotics Laboratory
Department of Computing, Imperial College London</p>
<h2 id="codeslam-&#x2014;-learning-a-compact-optimisable-representation-for-dense-visual-slam">CodeSLAM &#x2014; Learning a Compact, Optimisable Representation for Dense Visual SLAM</h2>
<h2 id="cubeslam-monocular-3d-object-detection-and-slam-without-prior-models">CubeSLAM: Monocular 3D Object Detection and SLAM without Prior Models</h2>
<ul>
<li>Authors: Shichao Yang, Sebastian Scherer</li>
<li>Robotics Institute, Carnegie Mellon University</li>
<li>{fshichaoy, bastig}\@andrew.cmu.edu</li>
<li><p><a href="https://github.com/shichaoy/cube_slam" target="_blank">Github-shichaoy/cube_slam</a>, with ROS</p>
<p>  This code contains a basic implementation for Cube SLAM. Given RGB and 2D object detection, the algorithm detects 3D cuboids from each frame then formulate an object SLAM to optimize both camera pose and cuboid poses.</p>
</li>
<li><a href="https://youtu.be/QnVlexXi9_c" target="_blank">youtube-demo</a></li>
</ul>
<iframe width="640" height="508" src="https://www.youtube.com/embed/QnVlexXi9_c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<pre><code class="lang-bibtex">@article{yang2018cubeslam,
  title={CubeSLAM: Monocular 3D Object Detection and SLAM without Prior Models},
  author={Yang, Shichao and Scherer, Sebastian},
  journal={arXiv preprint arXiv:1806.00557},
  year={2018}
}
</code></pre>
<h2 id="dynaslam-tracking-mapping-and-inpainting-in-dynamic-scenes">DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes</h2>
<ul>
<li><a href="https://github.com/BertaBescos/DynaSLAM" target="_blank">Github-BertaBescos/DynaSLAM</a></li>
<li><a href="https://bertabescos.github.io/DynaSLAM/" target="_blank">project</a></li>
<li><a href="https://youtu.be/EabI_goFmQs" target="_blank">Demo-Youtube</a></li>
</ul>
<iframe width="640" height="480" src="https://www.youtube.com/embed/EabI_goFmQs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<pre><code class="lang-bibtex">@inproceedings{bescos2018dynaslam,
  author = {Bescos, Berta and F{\&apos;a}cil, Jos{\&apos;e} M. and Civera, Javier and Neira, Jos{\&apos;e}},
  title = {{DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes}},
  journal = {arXiv},
  year = {2018}
}
</code></pre>
<h2 id="monocular-object-and-plane-slam-in-structured-environments">Monocular Object and Plane SLAM in Structured Environments</h2>
<ul>
<li><a href="https://arxiv.org/pdf/1809.03415.pdf" target="_blank">paper-pdf</a></li>
</ul>
<pre><code class="lang-bibtex">@article{yang2018monocular,
  title={Monocular Object and Plane SLAM in Structured Environments},
  author={Yang, Shichao and Scherer, Sebastian},
  journal={arXiv preprint arXiv:1809.03415},
  year={2018}
}
</code></pre>
<ul>
<li><a href="https://youtu.be/jzBMsKCm0uk" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="640" height="508" src="https://www.youtube.com/embed/jzBMsKCm0uk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>




<h2 id="deep-learning-based-semantic-labelling-of-3d-point-cloud-in-visual-slam">Deep Learning Based Semantic Labelling of 3D Point Cloud in Visual SLAM</h2>
<ul>
<li><a href="https://github.com/qixuxiang/orb-slam2_with_semantic_label" target="_blank">Github-qixuxiang/orb-slam2_with_semantic_label</a></li>
<li><a href="https://github.com/qixuxiang/real_time_object_detect" target="_blank">Github-qixuxiang/real_time_object_detect</a></li>
<li><a href="https://github.com/AmeyaWagh/3D_object_recognition" target="_blank">AmeyaWagh/3D_object_recognition</a>
 recognize and localize an object in 3D Point Cloud scene using 3D-CNNs</li>
</ul>
<pre><code class="lang-bibtex">@inproceedings{qi2018deep,
  title={Deep Learning Based Semantic Labelling of 3D Point Cloud in Visual SLAM},
  author={Qi, Xuxiang and Yang, Shaowu and Yan, Yuejin},
  booktitle={IOP Conference Series: Materials Science and Engineering},
  volume={428},
  number={1},
  pages={012023},
  year={2018},
  organization={IOP Publishing}
}
</code></pre>
<h2 id="cnn-svo-improving-the-mapping-in-semi-direct-visual-odometry-using-single-image-depth-prediction">CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction</h2>
<ul>
<li><a href="https://github.com/yan99033/CNN-SVO" target="_blank">Github-yan99033/CNN-SVO</a> support ROS</li>
<li><a href="https://github.com/yan99033/monodepth-cpp" target="_blank">Github-yan99033/monodepth-cpp</a>
Tensorflow C++ implementation for single image depth estimation</li>
<li><a href="https://www.youtube.com/watch?v=4oTzwoby3j" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="640" height="480" src="https://www.youtube.com/embed/4oTzwoby3jw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<h2 id="undeepvo-monocular-visual-odometry-through-unsupervised-deep-learning">UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning</h2>
<ul>
<li><a href="https://github.com/drmaj/UnDeepVO" target="_blank">Github-drmaj/UnDeepVO</a>
The implementation is not working yet. Once it is complete, the README will be updated with a full description and usage directions.</li>
<li>Paper: <a href="https://arxiv.org/pdf/1709.06841v1.pdf" target="_blank">https://arxiv.org/pdf/1709.06841v1.pdf</a></li>
<li>Project Website: <a href="http://senwang.gitlab.io/UnDeepVO/" target="_blank">http://senwang.gitlab.io/UnDeepVO/</a></li>
<li><a href="https://youtu.be/5RdjO93wJqo" target="_blank">youtube-Demo</a></li>
</ul>
<iframe width="640" height="555" src="https://www.youtube.com/embed/5RdjO93wJqo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<h2 id="unsupervised-learning-of-monocular-depth-estimation-and-visual-odometry-with-deep-feature-reconstruction">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</h2>
<pre><code class="lang-bibtex">@InProceedings{Zhan_2018_CVPR,
author = {Zhan, Huangying and Garg, Ravi and Saroj Weerasekera, Chamara and Li, Kejie and Agarwal, Harsh and Reid, Ian},
title = {Unsupervised Learning of Monocular Depth Estimation and Visual Odometry With Deep Feature Reconstruction},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}
</code></pre>
<ul>
<li><a href="Huangying-Zhan/Depth-VO-Feat">Github</a></li>
<li><a href="https://huangying-zhan.github.io/" target="_blank">Huangying Zhan / &#x7C98;&#x714C;&#x7192;</a></li>
</ul>
<h2 id="fast-and-accurate-semantic-mapping-through-geometric-based-incremental-segmentation">Fast and Accurate Semantic Mapping through Geometric-based Incremental Segmentation</h2>
<ul>
<li><a href="https://youtu.be/nYG8jqFBqWI" target="_blank">youtube-demo</a></li>
</ul>
<iframe width="640" height="480" src="https://www.youtube.com/embed/nYG8jqFBqWI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<h2 id="real-time-object-pose-estimation-with-pose-interpreter-networks">Real-Time Object Pose Estimation with Pose Interpreter Networks</h2>
<ul>
<li><a href="https://github.com/jimmyyhwu/pose-interpreter-networks" target="_blank">Github-jimmyyhwu/pose-interpreter-networks</a></li>
<li><a href="https://youtu.be/9QBw1NCOOR0" target="_blank">youtube-demo</a></li>
</ul>
<iframe width="640" height="480" src="https://www.youtube.com/embed/9QBw1NCOOR0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<pre><code class="lang-bibtex">@INPROCEEDINGS{wu2018pose,
author={J. Wu and B. Zhou and R. Russell and V. Kee and S. Wagner and M. Hebert and A. Torralba and D. M. S. Johnson},
booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
title={Real-Time Object Pose Estimation with Pose Interpreter Networks},
year={2018},
volume={},
number={},
pages={6798-6805},
keywords={Pose estimation;Image segmentation;Three-dimensional displays;Quaternions;Real-time systems;Training;Task analysis},
doi={10.1109/IROS.2018.8593662},
ISSN={2153-0866},
month={Oct},}
</code></pre>
<h2 id="bisenetbilateral-segmentation-network-for-real-time-semantic-segmentation">BiSeNet:Bilateral Segmentation Network for Real-time Semantic Segmentation</h2>
<ul>
<li><a href="https://github.com/Shuai-Xie/BiSeNet-CCP" target="_blank">Github-Shuai-Xie/BiSeNet-CCP</a></li>
</ul>
<h2 id="sfmlearner-learning-monocular-depth--ego-motion-using-meaningful-geometric-constraints">SfMLearner++: Learning Monocular Depth &amp; Ego-Motion using Meaningful Geometric Constraints</h2>
<ul>
<li><a href="https://arxiv.org/pdf/1812.08370.pdf" target="_blank">paper-pdf</a></li>
<li><a href="SfMLearner++: Learning Monocular Depth &amp; Ego-Motion using Meaningful Geometric Constraints" target="_blank">Githut-tinghuiz/SfMLearner</a></li>
</ul>
<h2 id="deep-virtual-stereo-odometry-leveraging-deep-depth-prediction-for-monocular-direct-sparse-odometry">Deep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry</h2>
<ul>
<li><a href="https://youtu.be/sLZOeC9z_tw" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="640" height="555" src="https://www.youtube.com/embed/sLZOeC9z_tw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



<h1 id="2017">2017</h1>
<h2 id="2017-fast-incremental-bundle-adjustment-with-covariance-recovery">2017-Fast Incremental Bundle Adjustment with Covariance Recovery</h2>
<ul>
<li><a href="https://youtu.be/PG9X9k7KXRo" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="640 " height="480" src="https://www.youtube.com/embed/PG9X9k7KXRo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



<h2 id="bundlefusion-real-time-globally-consistent-3d-reconstruction-using-on-the-fly-surface-reintegration">BundleFusion: Real-time Globally Consistent 3D Reconstruction using On the-fly Surface Reintegration</h2>
<ul>
<li><a href="https://youtu.be/keIirXrRb1k" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="640" height="555" src="https://www.youtube.com/embed/keIirXrRb1k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<ul>
<li><a href="http://graphics.stanford.edu/projects/bundlefusion/" target="_blank">Project</a></li>
<li><a href="https://github.com/niessner/BundleFusion" target="_blank">Github-niessner/BundleFusion</a></li>
</ul>
<h2 id="scenenet-rgb-d-can-5m-synthetic-images-beat-generic-imagenet-pre-training-on-indoor-segmentation">Scenenet rgb-d: Can 5m synthetic images beat generic imagenet pre-training on indoor segmentation</h2>
<pre><code class="lang-bibtex">@inproceedings{mccormac2017scenenet,
  title={Scenenet rgb-d: Can 5m synthetic images beat generic imagenet pre-training on indoor segmentation},
  author={McCormac, John and Handa, Ankur and Leutenegger, Stefan and Davison, Andrew J},
  booktitle={Proceedings of the International Conference on Computer Vision (ICCV)},
  volume={4},
  year={2017}
}
</code></pre>
<ul>
<li><a href="https://bitbucket.org/dysonroboticslab/scenenetrgb-d/src/master/" target="_blank">Bitbucket-Dyson Robotics Lab/SceneNetRGB-D</a></li>
</ul>
<h2 id="3dmatch-learning-local-geometric-descriptors-from-rgb-d-reconstructions">3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions</h2>
<ul>
<li><a href="https://arxiv.org/pdf/1603.08182.pdf" target="_blank">Paper-pdf</a></li>
</ul>
<pre><code class="lang-bibtex">@inproceedings{zeng20163dmatch,
    title={3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions},
    author={Zeng, Andy and Song, Shuran and Nie{\ss}ner, Matthias and Fisher, Matthew and Xiao, Jianxiong and Funkhouser, Thomas},
    booktitle={CVPR},
    year={2017}
}
</code></pre>
<ul>
<li><p><a href="http://3dmatch.cs.princeton.edu/" target="_blank">3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions</a></p>
</li>
<li><p><a href="https://github.com/andyzeng/3dmatch-toolbox" target="_blank">andyzeng/3dmatch-toolbox</a>
3DMatch is a ConvNet-based local geometric feature descriptor that operates on 3D data (i.e. point clouds, depth maps, meshes, etc.). This toolbox provides code to use 3DMatch for geometric registration and keypoint matching, as well as code to train 3DMatch from existing RGB-D reconstructions. This is the reference implementation of our paper:</p>
</li>
<li><p><a href="https://youtu.be/gZrsJJtDvvA" target="_blank">Demo-Youtube</a></p>
</li>
</ul>
<iframe width="640" height="480" src="https://www.youtube.com/embed/gZrsJJtDvvA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<ul>
<li><a href="https://youtu.be/qNVZl7bCjsU?list=PL_bDvITUYucADb15njRd7geem8vxOyo6N" target="_blank">Presentation-Youtube</a></li>
</ul>
<iframe width="640" height="480" src="https://www.youtube.com/embed/qNVZl7bCjsU?list=PL_bDvITUYucADb15njRd7geem8vxOyo6N" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>






<h2 id="semanticfusion-dense-3d-semantic-mapping-with-convolutional-neural-networks">SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks</h2>
<pre><code class="lang-bibtex">@inproceedings{mccormac2017semanticfusion,
  title={Semanticfusion: Dense 3d semantic mapping with convolutional neural networks},
  author={McCormac, John and Handa, Ankur and Davison, Andrew and Leutenegger, Stefan},
  booktitle={2017 IEEE International Conference on Robotics and automation (ICRA)},
  pages={4628--4635},
  year={2017},
  organization={IEEE}
}
</code></pre>
<ul>
<li><a href="https://bitbucket.org/dysonroboticslab/semanticfusion/src/master/" target="_blank">Bitbucket-Dyson Robotics Lab/SemanticFusion</a></li>
<li><a href="https://youtu.be/cGuoyNY54kU" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="640" height="604" src="https://www.youtube.com/embed/cGuoyNY54kU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>




<h1 id="2016">2016</h1>
<h2 id="2016-pairwise-decomposition-of-image-sequences-for-active-multi-view-recognition">2016-Pairwise Decomposition of Image Sequences for Active Multi-View Recognition</h2>
<pre><code class="lang-bibtex">@inproceedings{johns2016pairwise,
  title={Pairwise decomposition of image sequences for active multi-view recognition},
  author={Johns, Edward and Leutenegger, Stefan and Davison, Andrew J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3813--3822},
  year={2016}
}
</code></pre>
<ul>
<li><a href="https://youtu.be/7Bw0HGlidtg" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="640" height="480" src="https://www.youtube.com/embed/7Bw0HGlidtg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<h2 id="elasticfusion-real-time-dense-slam-and-light-source-estimation">ElasticFusion: Real-Time Dense SLAM and Light Source Estimation</h2>
<ul>
<li><a href="https://bitbucket.org/dysonroboticslab/semanticfusion/src/master/" target="_blank">Bitbucket-Dyson Robotics Lab/SemanticFusion</a></li>
<li><a href="https://github.com/mp3guy/ElasticFusion" target="_blank">mp3guy/ElasticFusion</a></li>
</ul>
<pre><code class="lang-bibtex">@article{whelan2016elasticfusion,
  title={ElasticFusion: Real-time dense SLAM and light source estimation},
  author={Whelan, Thomas and Salas-Moreno, Renato F and Glocker, Ben and Davison, Andrew J and Leutenegger, Stefan},
  journal={The International Journal of Robotics Research},
  volume={35},
  number={14},
  pages={1697--1716},
  year={2016},
  publisher={SAGE Publications Sage UK: London, England}
}
</code></pre>
<ul>
<li><a href="https://youtu.be/QFDnFjV9YdM" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="640" height="555" src="https://www.youtube.com/embed/QFDnFjV9YdM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



<h2 id="slam-with-objects-using-a-nonparametric-pose-graph">Slam with objects using a nonparametric pose graph</h2>
<ul>
<li><a href="https://github.com/BeipengMu/objectSLAM" target="_blank">Github-BeipengMu/objectSLAM</a></li>
<li><a href="http://people.csail.mit.edu/kaess/" target="_blank">Dr. Michael Kaess</a></li>
<li><a href="https://arxiv.org/pdf/1704.05959.pdf" target="_blank">paper-pdf</a></li>
<li><a href="https://youtu.be/YANUWdVLJD4" target="_blank">Demo-Youtube</a></li>
</ul>
<iframe width="640" height="480" src="https://www.youtube.com/embed/YANUWdVLJD4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<pre><code class="lang-bibtex">@inproceedings{mu2016slam,
  title={Slam with objects using a nonparametric pose graph},
  author={Mu, Beipeng and Liu, Shih-Yuan and Paull, Liam and Leonard, John and How, Jonathan P},
  booktitle={Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on},
  pages={4602--4609},
  year={2016},
  organization={IEEE}
}
</code></pre>
<h2 id="pop-up-slam-semantic-monocular-plane-slam-for-low-texture-environments">Pop-up SLAM: Semantic Monocular Plane SLAM for Low-texture Environments</h2>
<ul>
<li><p><a href="https://github.com/shichaoy/pop_up_slam" target="_blank">Github-shichaoy/pop_up_slam</a></p>
</li>
<li><p><a href="https://youtu.be/TOSOWdxmtkw" target="_blank">Youtube-Demo</a></p>
</li>
</ul>
<iframe width="640" height="480" src="https://www.youtube.com/embed/TOSOWdxmtkw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h2 id="real-time-3d-scene-layout-from-a-single-image-using-convolutional-neural-networks">Real-time 3D scene layout from a single image using convolutional neural networks</h2>
<ul>
<li><a href="https://www.ri.cmu.edu/pub_files/2016/5/icra_2016.pdf" target="_blank">Paper</a></li>
<li><a href="https://youtu.be/2CvFHy5jk1c" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="800" height="450" src="https://www.youtube.com/embed/2CvFHy5jk1c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<h1 id="2015">2015</h1>
<h2 id="slam-simultaneous-localisation-and-mapping-at-the-level-of-objecs">SLAM++: Simultaneous Localisation and Mapping at the Level of Objecs</h2>
<ul>
<li><a href="https://github.com/tolgabirdal/ppf_matching" target="_blank">Github-tolgabirdal/ppf_matching</a>
-</li>
</ul>
<h3 id="monocular-slam-supported-object-recognition">Monocular SLAM Supported Object Recognition</h3>
<ul>
<li><a href="https://people.csail.mit.edu/spillai/projects/vslam-object-recognition/pillai_rss15_poster.pdf" target="_blank">post</a></li>
<li><a href="http://people.csail.mit.edu/spillai/projects/vslam-object-recognition/" target="_blank">project</a></li>
<li><a href="http://www.roboticsproceedings.org/rss11/p34.pdf" target="_blank">paper</a></li>
</ul>
<h2 id="posenet-a-convolutional-network-for-real-time-6-dof-camera-relocalization">PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization</h2>
<ul>
<li><a href="https://github.com/alexgkendall/caffe-posenet" target="_blank">alexgkendall/caffe-posenet</a></li>
<li><a href="https://github.com/kentsommer/tensorflow-posenet" target="_blank">kentsommer/tensorflow-posenet</a></li>
<li><a href="https://youtu.be/u0MVbL_RyPU" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="640" height="555" src="https://www.youtube.com/embed/u0MVbL_RyPU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<h2 id="robot-in-a-room-toward-perfect-object-recognition-in-closed-environments">Robot In a Room: Toward Perfect Object Recognition in Closed Environments</h2>
<h1 id="2014">2014</h1>
<h2 id="semantic-localization-via-the-matrix-permanent">Semantic Localization Via the Matrix Permanent</h2>
<pre><code class="lang-bibtex">@inproceedings{atanasov2014semantic,
title={Semantic Localization Via the Matrix Permanent.},
author={Atanasov, Nikolay and Zhu, Menglong and Daniilidis, Kostas and Pappas, George J},
booktitle={Robotics: Science and Systems},
volume={2},
year={2014}
}
</code></pre>
<ul>
<li><a href="https://youtu.be/P3JuOT6kKM0" target="_blank">youtube-Demo</a></li>
</ul>
<iframe width="640" height="555" src="https://www.youtube.com/embed/P3JuOT6kKM0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h1 id="2013">2013</h1>
<h2 id="scene-coordinate-regression-forests-for-camera-relocalization-in-rgb-d-images">Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</h2>
<ul>
<li>[paper-pdf]</li>
</ul>
<pre><code class="lang-bibtex">@Inprceedings {export:184826,
author       = {Jamie Shotton and Ben Glocker and Christopher Zach and Shahram Izadi and Antonio
                Criminisi and Andrew Fitzgibbon},
booktitle    = {Proc. Computer Vision and Pattern Recognition (CVPR)},
month        = {June},
publisher    = {IEEE},
title        = {Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images},
url          = {http://research.microsoft.com/apps/pubs/default.aspx?id=184826},
year         = {2013},
}
</code></pre>
<ul>
<li><a href="https://github.com/ISUE/relocforests" target="_blank">Github-ISUE/relocforests</a></li>
</ul>
<h1 id="2012">2012</h1>
<h2 id="semantic-structure-from-motion-with-points-regions-and-objects">Semantic Structure From Motion with Points, Regions, and Objects</h2>
<ul>
<li><a href="https://youtu.be/rO6-mz1OVH8" target="_blank">Youtube-Demo</a></li>
</ul>
<iframe width="640" height="555" src="https://www.youtube.com/embed/rO6-mz1OVH8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<h1 id="2011">2011</h1>
<h2 id="kinectfusion-real-time-dense-surface-mapping-and-tracking">KinectFusion: Real-Time Dense Surface Mapping and Tracking</h2>
<ul>
<li><p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf" target="_blank">paper-pdf</a></p>
</li>
<li><p><a href="https://github.com/chrdiller/KinectFusionLib" target="_blank">chrdiller/KinectFusionLib</a></p>
</li>
<li><p><a href="https://youtu.be/KOUSSlKUJ-A" target="_blank">Youtube-Demo</a></p>
</li>
</ul>
<iframe width="640" height="480" src="https://www.youtube.com/embed/KOUSSlKUJ-A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<pre><code class="lang-bibtex">@inproceedings{newcombe2011kinectfusion,
  title={KinectFusion: Real-time dense surface mapping and tracking},
  author={Newcombe, Richard A and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J and Kohi, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
  booktitle={Mixed and augmented reality (ISMAR), 2011 10th IEEE international symposium on},
  pages={127--136},
  year={2011},
  organization={IEEE}
}
</code></pre>
<h2 id="towards-semantic-slam-using-a-monocular-camera">Towards Semantic SLAM using a Monocular Camera</h2>
<pre><code class="lang-bibtex">@inproceedings{civera2011towards,
title={Towards semantic SLAM using a monocular camera},
author={Civera, Javier and G{\&apos;a}lvez-L{\&apos;o}pez, Dorian and Riazuelo, Luis and Tard{\&apos;o}s, Juan D and Montiel, JMM},
booktitle={Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on},
pages={1277--1284},
year={2011},
organization={IEEE}
}
</code></pre>
<h2 id="ssfm-semantic-structure-from-motion">SSFM-Semantic Structure From Motion</h2>
<ul>
<li><a href="http://vhosts.eecs.umich.edu/vision//projects/ssfm/index.html" target="_blank">project</a></li>
</ul>
<pre><code class="lang-bibtex">@article{bao2011semantic,
  title={Semantic structure from motion},
  author={Bao, Sid Yingze and Savarese, Silvio},
  year={2011},
  publisher={IEEE}
}
</code></pre>
<h2 id="semantic-structure-from-motion-a-novel-framework-for-joint-object-recognition-and-3d-reconstruction">Semantic Structure from Motion: a Novel Framework for Joint Object Recognition and 3D Reconstruction</h2>
<h2 id="semantic-structure-from-motion-with-object-and-point-interactions">Semantic structure from motion with object and point interactions</h2>
<h1 id="object-recognization">Object Recognization</h1>
<h2 id="mask-r-cnn">Mask r-cnn</h2>
<ul>
<li><a href="https://github.com/qixuxiang/mask_rcnn_ros" target="_blank">Github-qixuxiang/mask_rcnn_ros</a> with ROS</li>
</ul>
<pre><code class="lang-bibtex">@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\&apos;a}r, Piotr and Girshick, Ross},
  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},
  pages={2980--2988},
  year={2017},
  organization={IEEE}
}
</code></pre>
<h2 id="2010-model-globally-match-locally-efficient-and-robust-3d-object-recognition">2010-Model globally, match locally: Efficient and robust 3D object recognition</h2>
<ul>
<li><a href="https://github.com/tolgabirdal/ppf_matching" target="_blank">Github-tolgabirdal/ppf_matching</a>
-</li>
</ul>
 <link rel="stylesheet" type="text/css" href="https://storage.googleapis.com/app.klipse.tech/css/codemirror.css"> <script>     window.klipse_settings = {         selector: ".language-klipse, .lang-eval-clojure",         selector_eval_js: ".lang-eval-js",         selector_eval_python_client: ".lang-eval-python",         selector_eval_php: ".lang-eval-php",         selector_eval_scheme: ".lang-eval-scheme",         selector_eval_ruby: ".lang-eval-ruby",         selector_reagent: ".lang-reagent",        selector_google_charts: ".lang-google-chart",        selector_es2017: ".lang-eval-es2017",        selector_jsx: ".lang-eval-jsx",        selector_transpile_jsx: ".lang-transpile-jsx",        selector_render_jsx: ".lang-render-jsx",        selector_react: ".lang-react",        selector_eval_markdown: ".lang-render-markdown",        selector_eval_lambdaway: ".lang-render-lambdaway",        selector_eval_cpp: ".lang-eval-cpp",        selector_eval_html: ".lang-render-html",        selector_sql: ".lang-eval-sql",        selector_brainfuck: "lang-eval-brainfuck",        selector_js: ".lang-transpile-cljs"    }; </script> <script src="https://storage.googleapis.com/app.klipse.tech/plugin/js/klipse_plugin.js"></script>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="Recursive_Bayes_Filter.html" class="navigation navigation-prev " aria-label="Previous page: Recursive Bayes Filter">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="slam-with-objects-using-a-nonparametric-pose-graph.html" class="navigation navigation-next " aria-label="Next page: Slam with objects using a nonparametric pose graph">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"AI SLAM Summary","level":"3.1","depth":1,"next":{"title":"Slam with objects using a nonparametric pose graph","level":"3.2","depth":1,"path":"slam-with-objects-using-a-nonparametric-pose-graph.md","ref":"slam-with-objects-using-a-nonparametric-pose-graph.md","articles":[]},"previous":{"title":"Recursive Bayes Filter","level":"2.2","depth":1,"path":"Recursive_Bayes_Filter.md","ref":"Recursive_Bayes_Filter.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{"_pictures":[{"backlink":"SLAM-Resources.html#fig1.3.1","level":"1.3","list_caption":"Figure: Georg Klein","alt":"Georg Klein","nro":1,"url":"http://www.robots.ox.ac.uk/~gk/imgs/georg_klein_136.jpg","index":1,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Georg Klein","attributes":{},"skip":false,"key":"1.3.1"},{"backlink":"SLAM++.html#fig3.4.1","level":"3.4","list_caption":"Figure: center 60%","alt":"center 60%","nro":2,"url":"/Assets/images/papers/SLAM++/SLAM++-RealTimeObjectDetection.png","index":1,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"center 60%","attributes":{},"skip":false,"key":"3.4.1"},{"backlink":"SLAM++.html#fig3.4.2","level":"3.4","list_caption":"Figure: 50% center","alt":"50% center","nro":3,"url":"/Assets/images/papers/SLAM++/ICP_breif_description.png","index":2,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"50% center","attributes":{},"skip":false,"key":"3.4.2"},{"backlink":"SLAM++.html#fig3.4.3","level":"3.4","list_caption":"Figure: center 80%","alt":"center 80%","nro":4,"url":"/Assets/images/papers/SLAM++/AugmentedRealityWithObjects.png","index":3,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"center 80%","attributes":{},"skip":false,"key":"3.4.3"},{"backlink":"kinect2.html#fig7.2.1","level":"7.2","list_caption":"Figure: Kinetic2 Demo","alt":"Kinetic2 Demo","nro":5,"url":"../Assets/Images/Kinetic2_Protonect_Test_Result.jpg","index":1,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Kinetic2 Demo","attributes":{},"skip":false,"key":"7.2.1"},{"backlink":"slam-course-robot-mapping.html#fig8.3.1","level":"8.3","list_caption":"Figure: Localization Example","alt":"Localization Example","nro":6,"url":"/assets/Estimate the Robot Pose.png","index":1,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Localization Example","attributes":{},"skip":false,"key":"8.3.1"},{"backlink":"slam-course-robot-mapping.html#fig8.3.2","level":"8.3","list_caption":"Figure: Localization Example","alt":"Localization Example","nro":7,"url":"/assets/Estimate Landmarks.png","index":2,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Localization Example","attributes":{},"skip":false,"key":"8.3.2"},{"backlink":"slam-course-robot-mapping.html#fig8.3.3","level":"8.3","list_caption":"Figure: Localization Example","alt":"Localization Example","nro":8,"url":"/assets/Screen Shot 2019-01-28 at 7.20.23.png","index":3,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Localization Example","attributes":{},"skip":false,"key":"8.3.3"},{"backlink":"slam-course-robot-mapping.html#fig8.3.4","level":"8.3","list_caption":"Figure: Uncertainty Representation","alt":"Uncertainty Representation","nro":9,"url":"/assets/Screen Shot 2019-01-28 at 8.14.04.png","index":4,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Uncertainty Representation","attributes":{},"skip":false,"key":"8.3.4"},{"backlink":"slam-course-robot-mapping.html#fig8.3.5","level":"8.3","list_caption":"Figure: SLAM in the Probabilistic World","alt":"SLAM in the Probabilistic World","nro":10,"url":"/assets/Screen Shot 2019-01-28 at 8.39.27.png","index":5,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"SLAM in the Probabilistic World","attributes":{},"skip":false,"key":"8.3.5"},{"backlink":"slam-course-robot-mapping.html#fig8.3.6","level":"8.3","list_caption":"Figure: Motion and Observation Model","alt":"Motion and Observation Model","nro":11,"url":"assets/markdown-img-paste-20190128111722554.png","index":6,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Motion and Observation Model","attributes":{},"skip":false,"key":"8.3.6"},{"backlink":"slam-course-robot-mapping.html#fig8.3.7","level":"8.3","list_caption":"Figure: Motion Model","alt":"Motion Model","nro":12,"url":"assets/markdown-img-paste-20190128112352359.png","index":7,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Motion Model","attributes":{},"skip":false,"key":"8.3.7"},{"backlink":"slam-course-robot-mapping.html#fig8.3.8","level":"8.3","list_caption":"Figure: Motion Model Example","alt":"Motion Model Example","nro":13,"url":"assets/markdown-img-paste-20190128111616903.png","index":8,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Motion Model Example","attributes":{},"skip":false,"key":"8.3.8"},{"backlink":"slam-course-robot-mapping.html#fig8.3.9","level":"8.3","list_caption":"Figure: Standard Odometry Model","alt":"Standard Odometry Model","nro":14,"url":"assets/markdown-img-paste-20190128112933104.png","index":9,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Standard Odometry Model","attributes":{},"skip":false,"key":"8.3.9"},{"backlink":"slam-course-robot-mapping.html#fig8.3.10","level":"8.3","list_caption":"Figure: Observation Model","alt":"Observation Model","nro":15,"url":"assets/markdown-img-paste-20190128114114463.png","index":10,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Observation Model","attributes":{},"skip":false,"key":"8.3.10"},{"backlink":"slam-course-robot-mapping.html#fig8.3.11","level":"8.3","list_caption":"Figure: Observation Model Examples","alt":"Observation Model Examples","nro":16,"url":"assets/markdown-img-paste-20190128114225690.png","index":11,"caption_template":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","label":"Observation Model Examples","attributes":{},"skip":false,"key":"8.3.11"}]},"plugins":["multipart","youtube","image-captions","disqus","donate","advanced-emoji","splitter","mermaid-gb3","puml","graph","chart","simple-page-toc","sitemap-general","todo","terminal","copy-code-button","include-csv","klipse","scripts","page-numbering","codeblock-filename","katex","livereload"],"pluginsConfig":{"include-csv":{},"disqus":{"useIdentifier":false,"shortName":"https-yubaoliu-gitbooks-io"},"youtube":{},"puml":{},"livereload":{},"simple-page-toc":{},"todo":{},"splitter":{},"scripts":{"files":["./myscript.js"]},"search":{},"multipart":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"graph":{},"donate":{"alipay":"","alipayText":"支付宝捐赠","button":"Donate","title":"","wechat":"https://i.imgur.com/nUAbMLG.png","wechatText":"微信捐赠"},"sitemap-general":{"prefix":"https://yubaoliu.gitbooks.io"},"katex":{},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mermaid-gb3":{},"codeblock-filename":{},"copy-code-button":{},"klipse":{"myConfigKey":"it's the default value"},"page-numbering":{"chapterFormat":"Chapter #chapno#&nbsp;&nbsp;&nbsp;#title# ||| #chapno#&nbsp;&nbsp;&nbsp;#title#","forceMultipleParts":false,"skipReadme":false},"advanced-emoji":{"embedEmojis":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"terminal":{"copyButtons":true,"fade":false,"style":"flat"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"chart":{"type":"c3"},"image-captions":{"caption":"Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_","variable_name":"_pictures"}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"AI-SLAM-Summary.md","mtime":"2019-01-29T01:24:37.213Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-01-30T02:01:12.728Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/URI.js/1.16.1/URI.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-disqus/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-donate/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-terminal/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-scripts/9f6c471c34065cdbabd862bf00229682-myscript.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    <script src="gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>

    </body>
</html>

